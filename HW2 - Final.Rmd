---
title: "HW2 - IMDB Analysis"
date: "2nd December 2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
---
Falomy Eugene (48686864), Syed Hassan Raza (48372523), Allen Mattam (48686903), Adhithya Madhavan (47705259), Srikanth Soma (48692883)


```{r setup, include=FALSE}
library(knitr)
knitr::opts_chunk$set(echo = FALSE)
library(reticulate) #Allows for Python to be run in chunks
```
```{r, include=FALSE}
#update.packages(ask = FALSE, checkBuilt = TRUE)
#tinytex::tlmgr_update()
```


```{python, eval = FALSE}
#py_install("scipy")
#py_install("seaborn")
#py_install("matplotlib")
#py_install("pandas")
#py_install("numpy")
 
 

import pandas as pd
import numpy as np
import seaborn as sns
from ast import literal_eval
from scipy.stats import norm
import warnings; warnings.simplefilter('ignore')

          
#Uploading all files in seperate dataframes

          

credits = pd.read_csv("C:/Users/efalo/OneDrive/Documents/SMU/SMU Docs/Fall Mod B/Applied Predictive Analytics 1/HW 2/Movie data/credits.csv")

Links = pd.read_csv("C:/Users/efalo/OneDrive/Documents/SMU/SMU Docs/Fall Mod B/Applied Predictive Analytics 1/HW 2/Movie data/links.csv")

meta_data = pd.read_csv("C:/Users/efalo/OneDrive/Documents/SMU/SMU Docs/Fall Mod B/Applied Predictive Analytics 1/HW 2/Movie data/movies_metadata.csv", parse_dates=['release_date'])

rating = pd.read_csv("C:/Users/efalo/OneDrive/Documents/SMU/SMU Docs/Fall Mod B/Applied Predictive Analytics 1/HW 2/Movie data/ratings.csv")


          
# #credits['cast'][0]
          

meta_data.shape
          
#just random checking
          
meta_data[meta_data['id'] == '110']
          
#the main data to work on
          
meta_data.head()
          
          
meta_data['belongs_to_collection'].count()
          
#Merging Different Data Together

          
meta_data.columns
          
#adding directors from credits dataframe
# ids are in object type and in order to merge, we need it in int form
          
credits['id'] = credits['id'].astype('int')
credits.head()
          
#data has id value that is date, so checking it and removing
#how did i know about it? while trying to join dataframes, I was getting error that id has dates as id value

          
meta_data[meta_data.id =='1997-08-20']
          
#removing it, in fact taking all other rows except these ones

          
meta_data = meta_data[meta_data.id !='1997-08-20'] 
meta_data = meta_data[meta_data.id !='2012-09-29'] 
meta_data = meta_data[meta_data.id !='2014-01-01']
          
#meta_data id is also in object form and needed us to delete certain rows that are mentioned in above chunk

          
meta_data['id'] = meta_data['id'].astype('int')
meta_data = meta_data.merge(credits, on='id')
meta_data.head()
          
#literal_eval is used for extracting dict values within string as in our case

          
meta_data['cast'] = meta_data['cast'].apply(literal_eval)
meta_data['crew'] = meta_data['crew'].apply(literal_eval)
          
##just the cast and crew size, that may be needed or not
#meta_data['cast_size'] = meta_data['cast'].apply(lambda x: len(x))
#meta_data['crew_size'] = meta_data['crew'].apply(lambda x: len(x))

#function for extracting only director's name
          
def get_director(x):
    for i in x:
        if i['job'] == 'Director':
            return i['name']
    return np.nan
          

#applying function and storing values in director column (new created) in meta_data
          
meta_data['director'] = meta_data['crew'].apply(get_director)
          

          
#parsing through dictionary and saving values for key = name in each row

meta_data['cast'] = meta_data['cast'].apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
#selecting top 2 actors
#lucky me, movies cast always has names in order of top actors
meta_data['cast'] = meta_data['cast'].apply(lambda x: x[:5] if len(x) >=5 else x)
          
          
#removing space from cast and all in lower alphabets
meta_data['cast'] = meta_data['cast'].apply(lambda x: [str.lower(i.replace(" ", "")) for i in x])
          
          
#saving 5 director names
meta_data['director'] = meta_data['director'].astype('str').apply(lambda x: str.lower(x.replace(" ", "")))
meta_data['director'] = meta_data['director'].apply(lambda x: [x])
          
          
#already extracted director from crew, so throwing it in wastebin
meta_data = meta_data.drop('crew', axis = 1)
meta_data.head()
          
#Data Structuring and Organizing (meta_data)
There are couple of variables that may not have use in our analysis. So we will be removing them.

Checking what columns to remove

          
meta_data['adult'].value_counts()  #column to remove: since almost all is false
          
          
#imdb_id is also not requried since movie id is already available
#check original_language
meta_data['original_language'].value_counts()  #five languages. Check if need to keep them all or just english
          
          
#original_title, spoken_languages also need to be removed
#checking status
meta_data['status'].value_counts() # we will only take those movies that are already released
          
          
#video means if dvd/cd or video version of that movie is released or not. it may add into as an additionally variable, results of which will interfer with model
meta_data['video'].value_counts() #take all that is false and remove true rows
          

#Now starting the cleaning Process:

columns to remove : 'adult' , 'imdb_id', 'homepage', 'original_title', 'spoken_languages', 'tagline'

Copying Useful Variables from meta_data dataframe to movies dataframe

          
movies = meta_data.drop(['adult' , 'imdb_id', 'homepage', 'original_title', 'overview', 'poster_path',
                         'spoken_languages', 'tagline'], axis = 1)

print(movies.head())
movies.describe()
          
          
#selecting only top 5 Languages
movies = movies[movies['original_language'].isin(['en', 'fr', 'it','ja','de'])]
          

          
#status = released only

movies = movies[movies['status'] == 'Released']
          
          
#Its ok to remove Triology, since individual movies are also in dataset
movies = movies[movies['video'] == False]
          
          
#removing all these worked out columns.
#Keeping belongs_to_collection for now
#'original_language': top 5 languages , 'status': released, 'video' : False, 'belongs_to_collection'
movies.drop(['original_language','status','video'], axis = 1, inplace = True)
movies.head()
          
#Extracting Data from Long Strings in column values Columns to work on:
#'belongs_to_collection', 'genres', 'production_companies', 'production_countries', year from 'release_date'

          
#release_year from release_date
movies['release_year'] = pd.DatetimeIndex(movies['release_date']).year
movies.head(2)
          

          
#I was getting float value for years, so..  #errors = 'ignore', in case of missing values it will ignore them
movies['release_year'] = movies['release_year'].astype('Int64', errors='ignore')
movies.head(2)
          
#Extracting Useful Variables from JSON Strings

          
#extracting genres
movies.genres[0:5][0]
          
          
movies['genres'] = movies['genres'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])

movies.head(2)
          
          
#production_companies
movies['production_companies'] = movies['production_companies'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
movies['production_companies'] = movies['production_companies'].apply(lambda x: x[:1] if len(x) >=1 else x)
          
          
movies['production_countries'] = movies['production_countries'].fillna('[]').apply(literal_eval).apply(lambda x: [i['name'] for i in x] if isinstance(x, list) else [])
movies['production_countries'] = movies['production_countries'].apply(lambda x: x[:1] if len(x) >=1 else x)
movies.head(2)
          
          
#converting belongs_to_collection to bool 1 & 0. If collection = 1 else 0
movies['belongs_to_collection'] = (movies['belongs_to_collection'].isna()).astype(int)

movies.head()
          
#Saving Plan for Uncertainities

          
mv_df = movies.copy()
mv_df.head()
          

#Data Cleaning

          
#checking missing values in data 
movies.isna().sum()

#since these are small values, we can remove these rows
          
          
movies.dropna(inplace = True)
movies.isna().sum()
          
          
#checking stats for outliers in continuous values
movies[['budget','popularity','revenue','runtime','vote_average','vote_count']].agg(['std','min','mean','max','skew','kurtosis'])

          
          
#there are some dodgy values or outliers to resolve in popularity, budget, runtime, vote_count
#start from popularity to check what values do we have and their type
(movies['popularity'].values)
          
#The values in popularity column have messed up data types. We see some float and some are string. We need to make all numerical values to be float

          
movies['popularity'] = movies['popularity'].astype("float")
print(movies['popularity'].values)
          

          
#checking popularity scale now
movies['popularity'].describe()  # its from zero to 547 with mean 6.92
          
          
#lets check out these super popular movie 547.488 score
movies[movies['popularity']> 40]  
#there are small number of movies above that popularity (40).
#so instead of making them outlier, set them all to 40
# all of them are popular,renowned movies

          

          
movies.popularity.mean() - movies.popularity.std() * 2, movies.popularity.mean() + movies.popularity.std() * 2
#since our range for outlier becomes small, lets make all above 40.0 in popularity as 40.0
          

          
#setting all movies with popularity above 40 to 40
#Professor Wayne you said this is ok
movies[movies['popularity'] > movies['popularity'].quantile(0.997)] = 40  #int(movies['popularity'].quantile(0.997))

    
movies.popularity.describe()
          

          
movies.shape
          

          
movies = movies[movies.popularity > 0.20]
movies.shape
          

          
movies.popularity.describe()
          
          
#lets look into next column: budget
movies['budget'].value_counts()   

#30K zeroes out of 45K data
          
          
#same problem and hence same solution
movies['budget'] = movies['budget'].astype(float)
#int because huge numerical values. no need to burden memory by using float here
print(movies['budget'].values)
          
#Now, there is possibility to create a movie with 0 budget, but not in real world so we have to look into this problem. since budgets really vary a lot in reality and taking them as outlier will be injustice

          
movies['budget'].describe() #check which movies have budget 0
          
          
#movies[movies['budget'] == movies['budget'].min()][0:5]
          
          
#checking our 2std or outlier range
movies.budget.mean() - (2 * movies.budget.std()) , movies.budget.mean() + 2 * movies.budget.std()
          
#The budget values in these records are not true which I checked online. The budget for them is not zero. So we may have to remove them from our record. (Check home Alone 4 and The human Centipede 2)

          
#movies['budget'] = movies[movies['budget'] < 1 ]
#movies = movies[movies['budget'] > 1] 
#movies.head()
          
          
sns.distplot(x = movies.budget, kde = True, fit = norm)
          
#Removing Budget here

          
movies = movies.drop('budget', axis = 1)
movies.head()
          
          
#check column: runtime. it seems fine but lets check it
movies['runtime'].describe()
#there are movies that are longer than 3 hours and max is 476 min (~8 hours). lets check what is that thing
#also, a movie is also zero minute which is obviously not a movie but an outlier
          
          
movies = movies[movies['runtime'] > 30]
movies['runtime'].describe()
          
          
#usually a movie is within 180 min or 3 hours. lets allow more liberty to older movies to have more than that time
#lets see if a movie is longer than 3.5 hours or 60*3.5 = 210
movies[movies['runtime'] > 240]
          
          
#Leviathan: The Story of Hellraiser and Hellbound: Hellraiser II has runtime : 476 min
#and its true, I verified it online on imdb and this cant be taken as outlier
#and same is true for some other movies too. data is correct and we cant take them as outlier

#lets see distribution of runtime and figure out something from there
sns.distplot(movies['runtime'].values)
          
          
from scipy.stats import norm
ax = sns.distplot(movies['runtime'], fit=norm, kde=False)
          
          
#probably we will need to put a cut off limit at 200, since 99.7% (3*std) of data lies within 30 to 190 min

#sadly we will have to let go the longer movies. they are not good enough for now
movies = movies[movies['runtime'] < 240]
movies['runtime'].describe()
          
          
#Checking vote_average
movies['vote_average'].describe()  #very good boy :)
          
          
#checking vote_count
movies['vote_count'].describe()
          
#There are couple of unappreciated movies and some are loved by everyone. lets see what they are.
#btw huge std :o
#important question: whats the difference between vote_count and vote_averge.
#and can we keep just one of them and let got the other one. found solution to this by Weighted Rating created later in feature engineering

          
#revenue col outliers
print(movies['revenue'].describe())
#removing outlier in this one
#checking dist of it
sns.distplot(movies['revenue'], fit = norm, kde = False) #left skewed

#wayne told us to remove this
          
#the results of revenue col are worrying! there are lot of values around 0 revenue. lets check data limits/range

          
movies['revenue'].mean() - (2 * movies['revenue'].std()) , movies['revenue'].mean() + (2 * movies['revenue'].std())

          
          
movies[movies.revenue < 1]['revenue'].count()  # 2617 values are zeros
          
          
#movies = movies[movies.revenue > 1000000]
#dropping revenues
          
#removed all rows where budget was zero total of 2804 rows

          
movies.revenue.describe()  #Now ok
          
#Checking Outliers Again

          
#checking stats for outliers in continuous values
movies[['popularity','revenue','runtime','vote_average','vote_count']].agg(['std','min','mean','max','skew','kurtosis'])
          

#Dont panic on vote_average and vote_count, we have found solution for that too.

#On the lower bound, we have already truncated the data since revenue cant be zero However, keeping in high revenue movie and checking if there is any outlier far away from distribution that can impact our results

          
#check how many values do we have and what movies are they
movies[movies.revenue > movies.revenue.mean() + 3 * movies.revenue.std()] #these are all HIT movies
          
          
# lets do the same treatment we did earlier, put their revenue equal to our boundry point of finding outlier 
# i.e. mean + 3 std value
#movies[movies.revenue > movies.revenue.mean() + 3 * movies.revenue.std()]['revenue'] = movies.revenue.mean()+3*movies.revenue.std()
#movies.revenue.describe()
          
          
#dist now
sns.distplot(x= movies['revenue'], kde  = True,  fit = norm)
          
          
movies.head() 
          
#Feautre Engineering
#Opening Up List with Single Values
#production_companies , production_countries and director

          
movies.production_companies = movies.production_companies.str[0]
movies.production_countries = movies.production_countries.str[0]
movies.director = movies.director.str[0]

movies.head()
          
#Creating New Column Called cast_score¶
#(Based on Number of Big Actors in a Movie)

          
#list of our favorite actors
actors = ['tomhanks', 'alpacino','robertdeniro','leonardodicaprio','jimcarrey','brucewillis',
         'johnnydepp','bradpitt','clinteastwood','willsmith','denzelwashington','tomcruise',
         'nicolascage','keanureeves','danielday-lewis','anthonyhopkins','robinwilliams','morganfreeman',
         'christianbale','hughjackman','mattdamon','woodyallen','jacknicholson','marlonbrando','dustinhoffman',
         'paulnewman','spencertracy','jacklemmon','michaelcaine','jamesstewart','robertduvall','seanpenn',
         'jeffbridges','genehackman','charleschaplin','benkingsley','russelcrowe','kevinspacey',
         'tommyleejones','seanconnery','christopherwalken','heathledger','jamiefoxx','joaquinphoenix',
         'colinfirth','matthewmcconaughey','garyoldman','edwardnorton','robertdowneyjr','liamneeson','melgibson',
         'harrisonford','samueljackson','benaffleck','ryangosling','ryanreynolds','jenniferlawrence','scarlettjohansson',
         'cateblanchett','jenniferaniston','galgadot','salmahayek','katewinslet','angelinjolie','annehathaway',
         'melissamccarthy','jackiechan','willferrell','dwaynejohnson','vandiesel','chadwickboseman']
          

          
#extract match count
def match_count(hero):
    if hero in actors:
        return 1
    else: 
        return 0

          
          
#copying movie data for experiments
cast_mv = movies.copy() 
cast_mv['cast_score'] = [0] * cast_mv.shape[0]

for index,row in cast_mv.iterrows():
    length = len((row['cast']))
    for i in range(length):
        val = match_count((row['cast'][i]))
        cast_mv.at[index, 'cast_score' ] += val
        

          
          
cast_mv[cast_mv.cast_score == 2]
          
          
cast_mv.cast_score.value_counts()
          
          
cast_mv.isna().sum()
          
#For Directors, creating director score
#We will create binary column telling us if a movie has a renonwed director or not

          
#directors =[]
cast_mv[cast_mv.director == 'paulthomasanderson']
          
          
directors =['ridleyscott','clinteastwood','martinscorsese','paulthomasanderson','davidfincher','joelcoen',
           'davidlynch','christophernolan','alexanderpayne','michaelhaneke','stevenspielberg','romanpolanski',
           'peterjackson','anglee','quentintarantino','darrenaronofsky','davidcronenberg','larsvontrier','mikeleigh',
           'jamescameron','dannyboyle','woodyallen']
          
          
def director_match(drc):
    if drc in directors:
        return 1
    else:
        return 0
          
          
cast_mv['director_score'] = [0] * cast_mv.shape[0]

for index,row in cast_mv.iterrows():
    v = director_match(row['director'])
    cast_mv.at[index, 'director_score' ] += v
       
       

#dr_df['director_score'].value_counts()
          
#Production Company Score
#For big production houses, we will assign 1 and for smaller 0

          
companies =['Warner Bros','Sony Pictures Animation','Sony Pictures Entertainment','Walt Disney','Universal Pictures',
           'Walt Disney Animation Studios','Walt Disney Pictures','Twentieth Century Fox Animation','Columbia Pictures',
           'Universal Pictures', 'Twentieth Century Fox Film Corporation','DreamWorks Animation','Miramax Films','Pixar Animation Studios',
           'DreamWorks SKG','DC Entertainment']
          
          
def company_match(c):
    if c in companies:
        return 1
    else:
        return 0
          
          
cast_mv['production_score'] = [0] * cast_mv.shape[0]

for index,row in cast_mv.iterrows():
    v = company_match(row['production_companies'])
    cast_mv.at[index, 'production_score' ] += v
    
          
          
cast_mv.head(2)
          
#One Hot Encoding are character/factor/categorical variables


# It is saying that Gen_movie is not defined here, not sure why

          
gen_movies.genres.values
          
          
gen_movies = cast_mv.copy()
#try is for one hot encoding to check
for index,row in gen_movies.iterrows():
    for genre in row.genres:
        gen_movies.at[index, genre] = 1
        print(gen_movies.at[index,genre])
gen_movies=gen_movies.fillna(0)
gen_movies.head()
          

          
gen_movies=gen_movies.fillna(0)
gen_movies.head()
          

          
cast_mv = gen_movies.copy()
          


#Creating Our Y Variable: Weighted Rating
#I found this formula online on Kaggle: (Its imdb weighted rating formula and I will be using some help from here, because i like that approach)

#𝑊𝑒𝑖𝑔ℎ𝑡𝑒𝑑𝑅𝑎𝑡𝑖𝑛𝑔(𝑊𝑅)=(𝑣/(𝑣+𝑚).𝑅)+(𝑚/(𝑣+𝑚).𝐶) 
#where,

#v is the number of votes for the movie
#m is the minimum votes required to be listed in the chart. We will keep 99.7% of values (3 * std)
#R is the average rating of the movie
#C is the mean vote across the whole report

          
#Convert datatype of cols: vote_count and vote_average into int 

#and find m = minium votes required to be listed on chart. 
#we will use 3 std values as we did above earlier
m = cast_mv[cast_mv['vote_count'].notnull()]['vote_count'].astype('int').quantile(0.997)

#and our mean of vote_average
C = cast_mv[cast_mv['vote_average'].notnull()]['vote_average'].astype('int').mean()


print("cut off value: ",m)
print("Mean of vote_average column: ",C)
          
          
def weighted_rating(x):
    v = x['vote_count']
    R = x['vote_average']
    return (v/(v+m) * R) + (m/(m+v) * C)

cast_mv['W_Rating'] = cast_mv.apply(weighted_rating, axis=1)
cast_mv = cast_mv.sort_values('W_Rating', ascending=False)
          
#Our rating and ranking will be based on this new col: W_Rating

          
cast_mv.W_Rating.describe()

          
#Final Cleaning¶
#Removing all unwanted variables

          
cast_mv.columns
          
          
cast_mv = cast_mv.drop(['id','release_date','genres'] , axis = 1)  #production_countries, production_companies, dir


```



```{r}
library(reticulate)
# py_install("pandas")
# py_install("numpy")
# py_install("matplotlib")
# py_install("seaborn")
# py_install("scipy")

```

```{python}
#import pandas as pd
#import numpy as np
#import seaborn as sns
#from ast import literal_eval
#from scipy.stats import norm
#import warnings; warnings.simplefilter('ignore')

```



```{r, include = FALSE, message=FALSE, warning=FALSE}
library(dplyr)
library(tidyr)
library(tidyverse)
library(ggplot2)
library(car)
library(broom)
library(GGally)
library(psych)
library(purrr)

library(e1071)
library(ggplot2)
library(ggExtra)
library(gridExtra)
library(pROC)
library(ROCit)
library(psych)
library(caret)
library(regclass)
library(rpart)
library(rpart.plot)
library(splitstackshape)
library(caTools)
library(reticulate)
library(sjPlot)


```

# Introduction
## *Our goal is to find out what makes a movie great.* 
In this report, we will be exploring the impact and the relationship between various factors to determine what makes a "good movie" given its rating score. To better understand the impact, we will consider factors such as the collection to which a movie belongs, the cast of a movie, the directors, the production house, the duration, and the popularity of the movie in question. This understanding will help us predict "good movies" and "bad movies". We will be looking at movies from 1906 to 2014.

# Executive Summary
Using the factors mentioned above, our model suggests that having a top movie director, cast (actors), and a top production house while producing a movie, has a significant influence on the success of a movie. We also noticed the popularity of a movie can be due to the marketing efforts that the movie crew implements prior to the release of the movie. A high popularity does not necessarily translate to a high rating. We will explore the marginal effect of each of these factors below.

## Our analysis has brought to realization that:
### 1. *The probability of having a good movie increases by 85% with a "good director".*
### 2. *The probability of having a good movie increases by 74% with a "good cast".*
### 3. *The probability of having a good movie increases by 60% with a "good production company".*

# Understanding the Analysis
## *Movie Classification*
To determine whether a movie is good or bad, we created an IMDB formula based rating which deals with or takes into consideration certain groups that could disproportionately influence the rating. The rating is a number between 1 and 10, and we decided that **any movie with a rating greater than or equal to 6 is considered a good movie, otherwise it is classified as a bad movie.**

## *Assumptions*
We developed a number of assumptions in the hopes of capturing the causal effect of each factors. According to Aristotle, "the whole is greater than the sum of its parts". We do realize that it's difficult to state that a certain factor has an absolute impact on the end result without considering the impact of all the factors collectively. We also do acknowledge that certain factors not included in our analysis may have an affect on our model(endogeneity). 

We assumed that the top 22 directors according to IMDB as "Good Directors" relative to other directors in the list. Also, we chose the top 70 actors according to IMDB ranking as "Good Actors". Finally, we chose the top 16 production houses as "Good Production Company".


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
## Marginal effects

#From this insight we can realize that pumping in excess money for marketing prior to the release of movies has a 59% impact of a movie being successful


mov <- read.csv("C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/Clean_HW2.csv")
names(mov)[1] <- 'id'
mov <- mov %>%  mutate(Verdict = if_else(mov$W_Rating >= 6, 1, 0))
head(mov,20)
```

```{r include=FALSE}
#
ggplot(data = mov, 
       aes(x=runtime, y= W_Rating)) +
  geom_point(color='darkblue')
```
```{r include=FALSE}

mov %>% ggplot( 
       aes(x=popularity, y= W_Rating, color= belongs_to_collection)) + geom_point(alpha =0.5)
  
library(reticulate)
#abline( lm(W_Rating~popularity, data= mov))
```

```{python, include = TRUE}
#import pandas as pd
#import seaborn as sns
#import matplotlib.pyplot as plt

#df = pd.read_csv("C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/Clean_HW2.csv")


#df = df[df.release_year >1900]
#sns.relplot(y='W_Rating' , x='release_year', hue= 'director_score',kind="line" ,data = df)
#plt.show()

#df = df[df.release_year >1900]
#sns.relplot(y='W_Rating' , x='release_year', hue= 'production_score',kind="line" ,data = df)
#plt.show()

```
# Directors/Production House and ratings 
## *Understanding Descriptive Relationship*

```{r, out.width = "200px", out.height="200px"}
library(knitr)
knitr::include_graphics(c("C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/Plots/Plots/legn_TOP_director.png", "C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/Plots/Plots/legn_TOP_Production.png"))

```

The graph on the left displays the rating of directors across the time period. The segment that we chose as "good directors" (orange line) have significantly outperformed other directors (blue line). Also, the graph on the right displays the rating of production houses across the time period. The "good production houses" (orange line) that we selected, also outperformed the other production houses (blue line). This shows a level of consistency with the assumptions that we made.

From a descriptive analysis, we segregated released movies given that movie in question is being produced by a certain type of production house and directed by a certain type of directors. We observed that the better the director and the production house, the better the rating of movies. We wish to further explore the probability of having a good movie while considering the impact of good directors and good production houses. 

```{python, include = FALSE}
#import pandas as pd
#import seaborn as sns
#import matplotlib.pyplot as plt

#df = pd.read_csv("C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/Clean_HW2.csv")


#df = df[df.release_year >1900]
#sns.relplot(y='W_Rating' , x='runtime', hue= 'production_score',kind="line" ,data = df)
#plt.show()

```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#mov <- read.csv("C:/Users/efalo/OneDrive/Documents/SMU/SMU Docs/Fall Mod B/Applied Predictive Analytics 1/HW 2/Movie data/Clean_HW2.csv")
#names(mov)[1] <- 'id'
#mov <- mov %>%  mutate(Verdict = if_else(mov$W_Rating >= 6, 1, 0))
#head(mov,20)
```
```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
str(mov)
```
```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
summary(mov)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
describe(mov)
```
```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
list <- colnames(mov[,c(2,3,6,7,14,15,16,37)])
list
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
head(mov[list])
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
multi.hist(mov[list], nrow=3,ncol=3,global=FALSE)
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
list2 <- colnames(mov[,c(2,3,6,7,14,15,16,37)])
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
##Let us check if there is any corelation

# ggpairs(mov[list2])
# ggsave("correlation.pdf",height=5)
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#There seems to be some correlation between revenue and verdict, higher correlation between vote count and Verdict(0.603) or vote average and Verdict (0.56)

plot(mov$belongs_to_collection,mov$W_Rating)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#So movies that have sequels are not necessarily successful. 

plot(mov$popularity,mov$revenue)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#More Popular movies have earned good revenue, some less popular movies also earned good revenue. However, the data undermines time factor in assessing the revenue. We may therefore adjust the revenue for time(inflation). 

plot(mov$vote_count,mov$W_rating)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#This appears strange. Many movies that have fewer votes are having a very high rating!! We need to identify the movies and the reasons.

plot(mov$popularity,mov$W_Rating)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#Create a new dataframe based on the specific cols we intended to input in our following model

df <- mov[,c("Verdict", "belongs_to_collection", "popularity","runtime", "cast_score", "director_score","production_score")]
str(df)
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#This step we partition the data into a training set (70%) and a validation set (30%)

set.seed(1234)
split <- sample.split(df, SplitRatio = 0.7)
train_data1 <- subset(df, split == "TRUE")
test_data1 <- subset(df, split == "FALSE")
```



```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#Based on the training data build logistic model 

logit <- glm(Verdict~., data = train_data1, family = "binomial")
summary(logit)

# This code will rank the variable by level of importance ( Not sure if that is needed)
varImp(logit)
```
## Regression Analysis


```{r, message=FALSE, warning=FALSE, fig.show='13.5', results='hide'}
#plot_model(logit, type="std", title="Positive/Negative Effect")
```
While considering our model, having a top director directing the movie, and a top production house producing it, featuring star actors, its popularity, and its runtime increases the likelihood of it to turn out to be a good movie. Whereas it does not help if the movie belongs to a movie series. 

```{r, out.width = "300px", out.height="300px",fig.align = 'center'}
library(sjPlot)
plot_model(logit,transform = "plogis", show.values = TRUE, value.offset = .3, title = "Probability of Occurences")
```

# Marginal Effect
To determine whether a movie is expected to be good movie, we considered six major factors, which will be explored below.  

We observe that there is a negative relationship between the fact that a movie belongs to a series and the rating of the movie. Based on our model, assuming that everything remains constant, the probability that a movie is bad is 57% if is part of a movie series. However, this goes against the common perception and there could be unaccounted factors playing into this or it is just that the count of bad rated movies belonging to series is higher than the popular ones.

Moreover, we notice that our model is highly consistent with what was anticipated earlier. The star actors in the movie, the director, and the producers of the movie have a significant influence on the success of the movie by 74% , 85%, and 60% respectively.




```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
tidy(logit) %>% mutate(estimate = round(estimate,2),
                         statistic = round(statistic,2),
                         p.value = round(p.value,3),
                         std.error = NULL)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

# This step provide the testing for the logit model as well as roc statistics

p_glm <- predict(logit,newdata = test_data1)
roc(test_data1$Verdict,p_glm)
plot(roc(test_data1$Verdict,p_glm))
```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

# Confusion Matrix

confusionmatrix<- confusion_matrix(logit,test_data1)
confusionmatrix
```



```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#This step we provide the accuracy, misclassification rate, true positive rate, false positive rate, specificity, precision, and prevalence statistics.

classifications <- predict(logit,newdata=test_data1)
#confusionmatrix <- confusion_matrix(classifications,test_data1$Verdict, positive = "1")
nb_cm <- data.frame(matrix(ncol = 1,nrow = 4))
rownames(nb_cm) <- c('accuracy', 'misclassification', 'specificity', 'precision')


colnames(nb_cm) <- 'values'
nb_cm['accuracy',1] <- (12860+43)/13042
nb_cm['misclassification',1] <- 1 - ((12860+43)/13042)
nb_cm['specificity',1] <- 43/(43+32)
nb_cm['precision',1] <- 12860/(12860+32)


nb_cm

# take away given the logistic regression. I will use the following formula...exp(B0 + B1x+ ...+BkXk) / 1+ exp(B0 + B1x+ ...+BkXk)
# note that, all the 7 variables used in the model aobve are significant because their p-values are less than 0.05 significant level.
# We have a (AUC of over 96), which means that our x variables have perfectly predicted y
# That also put our model accuracy to above 98%, that could mean that our model is over fitting

```








```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#For the Intercept
exp(-10.356980 ) # odds

exp(-10.356980 )/(1+exp(-10.356980 ))

```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

# Know that the intercept,which oftens labled as the constant, is the expexted value of y(verdict or good/bad rating) when all the x or independent variables are zero. We noticed  that the intercept is negative, that means that the odd of a movie being a good movie given our independent variable is lower than the baseline. In other words, the relationship is negative.

```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
# for belongs_to_collection

exp(-0.291088  ) # odds

exp(-0.291088 )/(1+exp(-0.291088 )) # 43%
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
# Given the coefiiciennt is negative, that means belong_to_collection ahs negative relatu=ionship with the dependent variable. In this case, verdict. The probability that a movie is caractegorized as a bad movie is 43 percent assuming everything remains constant.

```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
# For popularity
exp(0.365437  ) # odds

exp(0.365437  )/(1+exp(0.365437  ))

#Assuming all other variables remain fixed:
#The probabilty of a movie being a good movie is 59% given its popularity. 

#(in recommendation, we consider factor that can make a movie popular, for instance, we can elaborate on the fact that an influence actor can increase the likelihood of a movie's popularity and so on.)

```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
# For Run_time
exp( 0.023752) # odds

exp( 0.023752 )/(1+exp( 0.023752))

# 51% percent of time, a movie is rated good because of its runtime. (I think Hassan put a limitation on th runtime when cleaning, but sure to explin that.)

```



```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#For cat_score

exp(1.043093) # odds

exp( 1.043093 )/(1+exp( 1.043093))

# There is 84% increase in the odds of a movie being a good movie or rated as good movie given the who is in the cast. and the probability of that happening is 74%.

# (In our recommendation, we can talk about the importance of having a good cast, but take into consideration the cost of having certain people being part of the cast)

```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

#director_score

exp(1.714095) # odds

exp( 1.714095 )/(1+exp(1.714095))

# The probality of a movie being good given who's dirrecting it is 85%. 

#(Their might other factors influencing such result that is not in the model, that is what we would refer to as endogneity... maybe the more well known the director is, the more budget available for the director in question )

```



```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#For production_score 

exp(0.405110) # odds

exp(0.405110 )/(1+exp(0.405110))

# (Not sure what the production score is), the probability that a movie is a good movie given its "production score" is 60 %.

# I can elaborate more once I know what the production score is.

```


```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#browser()
```


########################################################################################################


```{r, out.width = "300px", out.height="300px",fig.align = 'center'}
plot <- mov %>% ggplot( 
       aes(x=popularity, y= W_Rating, color= Verdict)) + geom_point(alpha =0.5)

plot+scale_color_gradient2(midpoint=0.5, low="brown", high = "blue")+ xlab("Popularity") +ylab("Rating")
```

Another interesting observation we thought was worth mentioning; There are many movies that have very high popularity but have very low ratings and vice versa. Given that we use the calculated rating to determine whether a movie is good or bad, we assumed that this might be due to movies that are marketed by pushing a lot of money to create a lot of hype from the start, or by having a very high rated actor in the movie, but doesn't gain a lot of viewership after the movie is actually released. However, our model suggests that the popularity of a movie means that the movie is 59% probable to be a good movie. 

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
reglog <- glm(Verdict~belongs_to_collection+popularity+runtime+cast_score+director_score+production_score,data=df,family='binomial')
summary(reglog)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
tidy(reglog) %>% mutate(estimate = round(estimate,2),
                         statistic = round(statistic,2),
                         p.value = round(p.value,3),
                         std.error = NULL)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
regdf1=data.frame(belongs_to_collection = 1, popularity = 12, revenue=200000,runtime=150, cast_score =2,director_score =0,production_score=1)
regdf2=data.frame(belongs_to_collection = 0, popularity = 12, revenue=500000,runtime=150, cast_score =3,director_score =1,production_score=1)
regdf3=data.frame(belongs_to_collection = 1, popularity = 2, revenue=500000,runtime=150, cast_score =1,director_score =0,production_score=0)
regdf4=data.frame(belongs_to_collection = 1, popularity = 10, revenue=500000,runtime=150, cast_score =2,director_score =1,production_score=0)
predict(reglog,regdf1)
predict(reglog,regdf2)
predict(reglog,regdf3)
predict(reglog,regdf4)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
predict(reglog,regdf1,type='response')
predict(reglog,regdf2,type='response')
predict(reglog,regdf3,type='response')
predict(reglog,regdf4,type='response')
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
testBeta1 = data.frame(belongs_to_collection = 1, popularity = c(1,2,5,6,11,12), revenue=200000,runtime=150, cast_score =2,director_score =0,production_score=1)
predpop  = predict(reglog,testBeta1)
predpop
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
mvXyr <- mov %>% group_by(mov['release_year']) %>% summarize(Animation=sum(Animation), Action=sum(Action), Crime =sum(Crime), Thriller=sum(Thriller),sum(Comedy),sum(Drama),sum(Family),sum(Horror))
head(mvXyr)
```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}
#singlemov <- read.csv("C:/Users/allen/Documents/SMU/CLASSES/FALL/MOD B/MAST 6251 - Predictive Analytics/HW2/HW 2 Data/single_value_HW2.csv")

```

```{r, out.width = "300px", out.height="300px",fig.align = 'center'}
mov %>% ggplot( 
       aes(x=runtime, y= W_Rating, color= Verdict)) + geom_point(alpha =0.5)+ xlab("Runtime") +ylab("Rating")
```

# Recommendations 

There are certain important factors that have a huge influence on the success of a movie. Although great cast, production and direction will add value, they come at a high cost. Having a great director had a better impact than cast and production (according to our model) and could be a balanced trade off between expense and success factors. Interestingly, cost of hiring a good director is (in most cases) less than hiring a good actor. For example in Inception (top movie according to our model), the director Christopher Nolan made half (~27 million USD) compared to Leonardo Di Caprio (~50 million USD). Other good factors that contribute to the success of the movie:

1) Run time between 90-150 minutes
2) Genres:  Animation, Adventure and Family Movie (for great ROI)
3) Popularity: IMDB calculates this variable using its proprietary hidden formula. This variable is calculated before the release of the movie and influences the marketing and movie trailer plans. So, in other words we recommend analytics based approach on marketing. 

# Conclusion

Our goal was to predict whether a movie is good based on various factors. We used an IMDB ranking based formula, which takes into consideration groups that could disproportionately influence movie ratings. We observed that it is critical to have great directors to create vision, top actors and top production house for the movie's success, in that order. Popularity could be a factor we like to be cautious about as it could have correlation with other factors discussed above. However we must also keep in mind that there might be other factors that are not included in our model. 

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

```

```{r, message=FALSE, warning=FALSE, fig.show='hide', results='hide'}

```

